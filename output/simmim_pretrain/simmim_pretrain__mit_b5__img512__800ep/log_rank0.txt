[2022-11-11 16:45:48 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 16:45:48 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images/training
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 16:45:48 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f1838eb5610>
[2022-11-11 16:48:34 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 16:48:34 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 16:48:34 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f186de97610>
[2022-11-11 16:48:34 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 16:48:34 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:12:34 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:12:34 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:12:34 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f4ff33085e0>
[2022-11-11 17:12:34 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:12:34 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:12:35 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:12:35 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:12:35 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:12:35 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:12:35 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:12:35 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:12:35 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:12:35 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:12:35 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:12:35 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:12:35 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:14:15 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:14:15 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:14:15 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f007c81ef70>
[2022-11-11 17:14:16 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:14:16 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:14:16 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:14:16 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:14:16 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:14:16 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:14:16 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:14:16 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:14:16 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:14:17 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:14:17 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:14:17 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:14:17 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:15:33 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:15:33 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:15:33 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f5f89052f10>
[2022-11-11 17:15:33 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:15:33 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:15:33 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:15:33 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:15:33 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:15:33 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:15:33 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:15:33 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:15:33 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:15:34 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:15:34 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:15:34 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:15:34 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:20:13 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:20:13 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:20:13 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fb90deb4c70>
[2022-11-11 17:20:13 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:20:13 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:20:14 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:20:14 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:20:14 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:20:14 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:20:14 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:20:14 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:20:14 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:20:14 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:20:14 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:20:14 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:20:14 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:22:11 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:22:11 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:22:11 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fcc2de64f40>
[2022-11-11 17:22:12 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:22:12 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:22:12 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:22:12 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:22:12 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:22:12 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:22:12 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:22:12 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:22:12 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:22:13 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:22:13 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:22:13 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:22:13 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:24:25 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:24:26 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:24:26 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f4e98034cd0>
[2022-11-11 17:24:26 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:24:26 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:24:26 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:24:26 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:24:26 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:24:26 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:24:26 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:24:26 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:24:26 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:24:27 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:24:27 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:24:27 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:24:27 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:25:11 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:25:11 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:25:11 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f9d9d29dcd0>
[2022-11-11 17:25:11 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:25:11 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:25:12 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:25:12 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:25:12 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:25:12 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:25:12 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:25:12 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:25:12 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:25:12 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:25:12 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:25:12 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:25:12 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:26:43 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:26:43 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:26:43 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f8cec900cd0>
[2022-11-11 17:26:43 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:26:43 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:26:44 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:26:44 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:26:44 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:26:44 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:26:44 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:26:44 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:26:44 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:26:44 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:26:44 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:26:44 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:26:44 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:29:48 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:29:48 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:29:48 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f93d051ecd0>
[2022-11-11 17:29:48 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:29:48 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:29:48 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:29:48 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:29:48 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:29:48 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:29:48 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:29:48 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:29:48 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:29:49 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:29:49 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:29:49 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:29:49 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:30:40 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:30:40 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:30:40 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fb11b52dcd0>
[2022-11-11 17:30:40 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:30:40 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:30:41 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:30:41 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:30:41 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:30:41 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:30:41 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:30:41 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:30:41 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:30:41 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:30:41 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:30:41 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:30:41 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:33:54 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:33:54 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:33:54 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fe04f36b610>
[2022-11-11 17:33:54 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:33:54 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:33:55 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:33:55 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:33:55 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:33:55 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:33:55 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:33:55 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:33:55 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:33:55 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:33:55 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:33:55 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:33:55 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:34:51 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:34:51 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:34:51 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f3cff20c610>
[2022-11-11 17:34:51 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:34:51 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:34:52 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:34:52 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:34:52 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:34:52 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:34:52 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:34:52 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:34:52 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:34:52 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:34:52 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:34:52 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:34:52 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:35:09 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f1c979bf610>
[2022-11-11 17:35:09 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:35:09 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:35:09 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:35:09 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:35:09 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:35:09 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:35:09 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:35:09 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:35:09 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:35:32 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:35:32 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:35:32 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fd22d417610>
[2022-11-11 17:35:33 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:35:33 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:35:33 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:35:33 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:35:33 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:35:33 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:35:33 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:35:33 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:35:33 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:35:33 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:35:33 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:35:33 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:35:33 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B3
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:35:58 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f78c0c42610>
[2022-11-11 17:35:58 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:35:58 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:35:58 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:35:58 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:35:58 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:35:58 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:35:58 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 83): INFO number of params: 45651136
[2022-11-11 17:35:58 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:35:58 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 128
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B3
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:36:19 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f1b9e2a4610>
[2022-11-11 17:36:19 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:36:19 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:36:19 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:36:19 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:36:19 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:36:19 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:36:19 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 83): INFO number of params: 45651136
[2022-11-11 17:36:19 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:36:19 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 64
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B3
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:36:36 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7effb3716610>
[2022-11-11 17:36:36 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:36:36 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:36:36 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:36:36 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:36:36 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:36:36 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:36:36 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 83): INFO number of params: 45651136
[2022-11-11 17:36:36 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:36:36 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:36:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][0/173]	eta 0:09:05 lr 0.000000	time 3.1551 (3.1551)	loss 0.3384 (0.3384)	grad_norm 0.1455 (0.1455)	mem 2101MB
[2022-11-11 17:36:43 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][10/173]	eta 0:01:37 lr 0.000000	time 0.3396 (0.5969)	loss 0.3299 (0.3335)	grad_norm 0.1376 (0.1384)	mem 2636MB
[2022-11-11 17:36:46 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][20/173]	eta 0:01:12 lr 0.000000	time 0.3388 (0.4737)	loss 0.3268 (0.3335)	grad_norm 0.1357 (0.1352)	mem 2636MB
[2022-11-11 17:36:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][30/173]	eta 0:01:01 lr 0.000001	time 0.3415 (0.4321)	loss 0.3230 (0.3331)	grad_norm 0.1200 (0.1307)	mem 2636MB
[2022-11-11 17:36:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][40/173]	eta 0:00:54 lr 0.000001	time 0.3377 (0.4100)	loss 0.3315 (0.3323)	grad_norm 0.1028 (0.1251)	mem 2636MB
[2022-11-11 17:36:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][50/173]	eta 0:00:48 lr 0.000001	time 0.3430 (0.3966)	loss 0.3267 (0.3320)	grad_norm 0.0974 (0.1201)	mem 2636MB
[2022-11-11 17:37:32 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:37:32 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 64
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:37:32 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f2d9addd610>
[2022-11-11 17:37:32 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:37:32 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:37:33 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:37:33 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:37:33 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:37:33 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:37:33 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:37:33 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:37:33 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:37:33 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:37:33 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:37:33 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:37:33 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:37:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][0/173]	eta 0:09:02 lr 0.000000	time 3.1357 (3.1357)	loss 0.3369 (0.3369)	grad_norm 0.1525 (0.1525)	mem 3582MB
[2022-11-11 17:37:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][10/173]	eta 0:02:12 lr 0.000000	time 0.5780 (0.8130)	loss 0.3331 (0.3329)	grad_norm 0.1431 (0.1445)	mem 4551MB
[2022-11-11 17:37:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][20/173]	eta 0:01:47 lr 0.000000	time 0.5735 (0.7021)	loss 0.3245 (0.3326)	grad_norm 0.1474 (0.1422)	mem 4551MB
[2022-11-11 17:38:01 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:38:01 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 128
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:38:01 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f4716c79610>
[2022-11-11 17:38:01 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:38:01 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:38:02 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:38:02 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:38:02 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:38:02 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:38:02 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:38:02 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:38:02 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:38:02 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:38:02 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:38:02 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:38:02 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:38:43 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:38:43 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 128
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:38:43 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f2a2e9fbf40>
[2022-11-11 17:38:43 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:38:43 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:38:43 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:38:43 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:38:43 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:38:43 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:38:43 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:38:43 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:38:43 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:38:44 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:38:44 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:38:44 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:38:44 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:40:38 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:40:38 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 64
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 2.5e-05
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05

[2022-11-11 17:40:38 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f386702ef40>
[2022-11-11 17:40:38 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:40:38 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:40:38 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:40:38 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:40:38 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:40:38 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:40:38 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:40:38 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:40:38 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 2.5e-05
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:40:39 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:40:39 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:40:39 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:40:39 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 8
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 1.5625e-06
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 7.8125e-08
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 7.8125e-09
  WEIGHT_DECAY: 0.05

[2022-11-11 17:50:01 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f82441f1610>
[2022-11-11 17:50:01 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:50:01 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:50:01 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:50:01 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:50:01 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:50:01 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:50:01 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 1.5625e-06
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 1.5625e-06
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:50:01 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:50:01 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:50:31 simmim_pretrain] (main_simmim.py 239): INFO Full config saved to /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep/config.json
[2022-11-11 17:50:31 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O0
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 1
  DATASET: imagenet
  DATA_PATH: /home/r/SimMIM-main/ADEChallengeData2016/images
  IMG_SIZE: 512
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  MIT:
    OUTPUT_DIM: 512
    SIZE: B5
  NAME: simmim_pretrain
  RESUME: ''
  TYPE: mit
OUTPUT: /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep
PRETRAINED: ''
PRINT_FREQ: 10
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__mit_b5__img512__800ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 1.953125e-07
  CLIP_GRAD: 5.0
  EPOCHS: 800
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS:
    - 700
    NAME: multistep
  MIN_LR: 9.765625e-09
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 9.765625e-10
  WEIGHT_DECAY: 0.05

[2022-11-11 17:50:31 simmim_pretrain] (data_simmim.py 85): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fc713e58610>
[2022-11-11 17:50:31 simmim_pretrain] (data_simmim.py 88): INFO Build dataset: train images = 22210
[2022-11-11 17:50:31 simmim_pretrain] (main_simmim.py 71): INFO Creating model:mit/simmim_pretrain
[2022-11-11 17:50:32 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): MitForSimMIM(
    (patch_embed1): PatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): PatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): PatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): PatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (fc2): Linear(in_features=256, out_features=64, bias=True)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (fc2): Linear(in_features=512, out_features=128, bias=True)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (3): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (4): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (5): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (6): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (7): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (8): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (9): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (10): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (11): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (12): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (13): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (14): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (15): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (16): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (17): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (18): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (19): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (20): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (21): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (22): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (23): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (24): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (25): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (26): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (27): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (28): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (29): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (30): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (31): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (32): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (33): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (34): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (35): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (36): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (37): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (38): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
      (39): Block(
        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Sequential(
    (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-11-11 17:50:32 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-11-11 17:50:32 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {}
[2022-11-11 17:50:32 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {}
[2022-11-11 17:50:32 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.patch_embed1.proj.bias', 'encoder.patch_embed1.norm.weight', 'encoder.patch_embed1.norm.bias', 'encoder.patch_embed2.proj.bias', 'encoder.patch_embed2.norm.weight', 'encoder.patch_embed2.norm.bias', 'encoder.patch_embed3.proj.bias', 'encoder.patch_embed3.norm.weight', 'encoder.patch_embed3.norm.bias', 'encoder.patch_embed4.proj.bias', 'encoder.patch_embed4.norm.weight', 'encoder.patch_embed4.norm.bias', 'encoder.block1.0.norm1.weight', 'encoder.block1.0.norm1.bias', 'encoder.block1.0.attn.q.bias', 'encoder.block1.0.attn.kv.bias', 'encoder.block1.0.attn.proj.bias', 'encoder.block1.0.attn.sr.bias', 'encoder.block1.0.attn.norm.weight', 'encoder.block1.0.attn.norm.bias', 'encoder.block1.0.norm2.weight', 'encoder.block1.0.norm2.bias', 'encoder.block1.0.mlp.fc1.bias', 'encoder.block1.0.mlp.dwconv.dwconv.bias', 'encoder.block1.0.mlp.fc2.bias', 'encoder.block1.1.norm1.weight', 'encoder.block1.1.norm1.bias', 'encoder.block1.1.attn.q.bias', 'encoder.block1.1.attn.kv.bias', 'encoder.block1.1.attn.proj.bias', 'encoder.block1.1.attn.sr.bias', 'encoder.block1.1.attn.norm.weight', 'encoder.block1.1.attn.norm.bias', 'encoder.block1.1.norm2.weight', 'encoder.block1.1.norm2.bias', 'encoder.block1.1.mlp.fc1.bias', 'encoder.block1.1.mlp.dwconv.dwconv.bias', 'encoder.block1.1.mlp.fc2.bias', 'encoder.block1.2.norm1.weight', 'encoder.block1.2.norm1.bias', 'encoder.block1.2.attn.q.bias', 'encoder.block1.2.attn.kv.bias', 'encoder.block1.2.attn.proj.bias', 'encoder.block1.2.attn.sr.bias', 'encoder.block1.2.attn.norm.weight', 'encoder.block1.2.attn.norm.bias', 'encoder.block1.2.norm2.weight', 'encoder.block1.2.norm2.bias', 'encoder.block1.2.mlp.fc1.bias', 'encoder.block1.2.mlp.dwconv.dwconv.bias', 'encoder.block1.2.mlp.fc2.bias', 'encoder.norm1.weight', 'encoder.norm1.bias', 'encoder.block2.0.norm1.weight', 'encoder.block2.0.norm1.bias', 'encoder.block2.0.attn.q.bias', 'encoder.block2.0.attn.kv.bias', 'encoder.block2.0.attn.proj.bias', 'encoder.block2.0.attn.sr.bias', 'encoder.block2.0.attn.norm.weight', 'encoder.block2.0.attn.norm.bias', 'encoder.block2.0.norm2.weight', 'encoder.block2.0.norm2.bias', 'encoder.block2.0.mlp.fc1.bias', 'encoder.block2.0.mlp.dwconv.dwconv.bias', 'encoder.block2.0.mlp.fc2.bias', 'encoder.block2.1.norm1.weight', 'encoder.block2.1.norm1.bias', 'encoder.block2.1.attn.q.bias', 'encoder.block2.1.attn.kv.bias', 'encoder.block2.1.attn.proj.bias', 'encoder.block2.1.attn.sr.bias', 'encoder.block2.1.attn.norm.weight', 'encoder.block2.1.attn.norm.bias', 'encoder.block2.1.norm2.weight', 'encoder.block2.1.norm2.bias', 'encoder.block2.1.mlp.fc1.bias', 'encoder.block2.1.mlp.dwconv.dwconv.bias', 'encoder.block2.1.mlp.fc2.bias', 'encoder.block2.2.norm1.weight', 'encoder.block2.2.norm1.bias', 'encoder.block2.2.attn.q.bias', 'encoder.block2.2.attn.kv.bias', 'encoder.block2.2.attn.proj.bias', 'encoder.block2.2.attn.sr.bias', 'encoder.block2.2.attn.norm.weight', 'encoder.block2.2.attn.norm.bias', 'encoder.block2.2.norm2.weight', 'encoder.block2.2.norm2.bias', 'encoder.block2.2.mlp.fc1.bias', 'encoder.block2.2.mlp.dwconv.dwconv.bias', 'encoder.block2.2.mlp.fc2.bias', 'encoder.block2.3.norm1.weight', 'encoder.block2.3.norm1.bias', 'encoder.block2.3.attn.q.bias', 'encoder.block2.3.attn.kv.bias', 'encoder.block2.3.attn.proj.bias', 'encoder.block2.3.attn.sr.bias', 'encoder.block2.3.attn.norm.weight', 'encoder.block2.3.attn.norm.bias', 'encoder.block2.3.norm2.weight', 'encoder.block2.3.norm2.bias', 'encoder.block2.3.mlp.fc1.bias', 'encoder.block2.3.mlp.dwconv.dwconv.bias', 'encoder.block2.3.mlp.fc2.bias', 'encoder.block2.4.norm1.weight', 'encoder.block2.4.norm1.bias', 'encoder.block2.4.attn.q.bias', 'encoder.block2.4.attn.kv.bias', 'encoder.block2.4.attn.proj.bias', 'encoder.block2.4.attn.sr.bias', 'encoder.block2.4.attn.norm.weight', 'encoder.block2.4.attn.norm.bias', 'encoder.block2.4.norm2.weight', 'encoder.block2.4.norm2.bias', 'encoder.block2.4.mlp.fc1.bias', 'encoder.block2.4.mlp.dwconv.dwconv.bias', 'encoder.block2.4.mlp.fc2.bias', 'encoder.block2.5.norm1.weight', 'encoder.block2.5.norm1.bias', 'encoder.block2.5.attn.q.bias', 'encoder.block2.5.attn.kv.bias', 'encoder.block2.5.attn.proj.bias', 'encoder.block2.5.attn.sr.bias', 'encoder.block2.5.attn.norm.weight', 'encoder.block2.5.attn.norm.bias', 'encoder.block2.5.norm2.weight', 'encoder.block2.5.norm2.bias', 'encoder.block2.5.mlp.fc1.bias', 'encoder.block2.5.mlp.dwconv.dwconv.bias', 'encoder.block2.5.mlp.fc2.bias', 'encoder.norm2.weight', 'encoder.norm2.bias', 'encoder.block3.0.norm1.weight', 'encoder.block3.0.norm1.bias', 'encoder.block3.0.attn.q.bias', 'encoder.block3.0.attn.kv.bias', 'encoder.block3.0.attn.proj.bias', 'encoder.block3.0.attn.sr.bias', 'encoder.block3.0.attn.norm.weight', 'encoder.block3.0.attn.norm.bias', 'encoder.block3.0.norm2.weight', 'encoder.block3.0.norm2.bias', 'encoder.block3.0.mlp.fc1.bias', 'encoder.block3.0.mlp.dwconv.dwconv.bias', 'encoder.block3.0.mlp.fc2.bias', 'encoder.block3.1.norm1.weight', 'encoder.block3.1.norm1.bias', 'encoder.block3.1.attn.q.bias', 'encoder.block3.1.attn.kv.bias', 'encoder.block3.1.attn.proj.bias', 'encoder.block3.1.attn.sr.bias', 'encoder.block3.1.attn.norm.weight', 'encoder.block3.1.attn.norm.bias', 'encoder.block3.1.norm2.weight', 'encoder.block3.1.norm2.bias', 'encoder.block3.1.mlp.fc1.bias', 'encoder.block3.1.mlp.dwconv.dwconv.bias', 'encoder.block3.1.mlp.fc2.bias', 'encoder.block3.2.norm1.weight', 'encoder.block3.2.norm1.bias', 'encoder.block3.2.attn.q.bias', 'encoder.block3.2.attn.kv.bias', 'encoder.block3.2.attn.proj.bias', 'encoder.block3.2.attn.sr.bias', 'encoder.block3.2.attn.norm.weight', 'encoder.block3.2.attn.norm.bias', 'encoder.block3.2.norm2.weight', 'encoder.block3.2.norm2.bias', 'encoder.block3.2.mlp.fc1.bias', 'encoder.block3.2.mlp.dwconv.dwconv.bias', 'encoder.block3.2.mlp.fc2.bias', 'encoder.block3.3.norm1.weight', 'encoder.block3.3.norm1.bias', 'encoder.block3.3.attn.q.bias', 'encoder.block3.3.attn.kv.bias', 'encoder.block3.3.attn.proj.bias', 'encoder.block3.3.attn.sr.bias', 'encoder.block3.3.attn.norm.weight', 'encoder.block3.3.attn.norm.bias', 'encoder.block3.3.norm2.weight', 'encoder.block3.3.norm2.bias', 'encoder.block3.3.mlp.fc1.bias', 'encoder.block3.3.mlp.dwconv.dwconv.bias', 'encoder.block3.3.mlp.fc2.bias', 'encoder.block3.4.norm1.weight', 'encoder.block3.4.norm1.bias', 'encoder.block3.4.attn.q.bias', 'encoder.block3.4.attn.kv.bias', 'encoder.block3.4.attn.proj.bias', 'encoder.block3.4.attn.sr.bias', 'encoder.block3.4.attn.norm.weight', 'encoder.block3.4.attn.norm.bias', 'encoder.block3.4.norm2.weight', 'encoder.block3.4.norm2.bias', 'encoder.block3.4.mlp.fc1.bias', 'encoder.block3.4.mlp.dwconv.dwconv.bias', 'encoder.block3.4.mlp.fc2.bias', 'encoder.block3.5.norm1.weight', 'encoder.block3.5.norm1.bias', 'encoder.block3.5.attn.q.bias', 'encoder.block3.5.attn.kv.bias', 'encoder.block3.5.attn.proj.bias', 'encoder.block3.5.attn.sr.bias', 'encoder.block3.5.attn.norm.weight', 'encoder.block3.5.attn.norm.bias', 'encoder.block3.5.norm2.weight', 'encoder.block3.5.norm2.bias', 'encoder.block3.5.mlp.fc1.bias', 'encoder.block3.5.mlp.dwconv.dwconv.bias', 'encoder.block3.5.mlp.fc2.bias', 'encoder.block3.6.norm1.weight', 'encoder.block3.6.norm1.bias', 'encoder.block3.6.attn.q.bias', 'encoder.block3.6.attn.kv.bias', 'encoder.block3.6.attn.proj.bias', 'encoder.block3.6.attn.sr.bias', 'encoder.block3.6.attn.norm.weight', 'encoder.block3.6.attn.norm.bias', 'encoder.block3.6.norm2.weight', 'encoder.block3.6.norm2.bias', 'encoder.block3.6.mlp.fc1.bias', 'encoder.block3.6.mlp.dwconv.dwconv.bias', 'encoder.block3.6.mlp.fc2.bias', 'encoder.block3.7.norm1.weight', 'encoder.block3.7.norm1.bias', 'encoder.block3.7.attn.q.bias', 'encoder.block3.7.attn.kv.bias', 'encoder.block3.7.attn.proj.bias', 'encoder.block3.7.attn.sr.bias', 'encoder.block3.7.attn.norm.weight', 'encoder.block3.7.attn.norm.bias', 'encoder.block3.7.norm2.weight', 'encoder.block3.7.norm2.bias', 'encoder.block3.7.mlp.fc1.bias', 'encoder.block3.7.mlp.dwconv.dwconv.bias', 'encoder.block3.7.mlp.fc2.bias', 'encoder.block3.8.norm1.weight', 'encoder.block3.8.norm1.bias', 'encoder.block3.8.attn.q.bias', 'encoder.block3.8.attn.kv.bias', 'encoder.block3.8.attn.proj.bias', 'encoder.block3.8.attn.sr.bias', 'encoder.block3.8.attn.norm.weight', 'encoder.block3.8.attn.norm.bias', 'encoder.block3.8.norm2.weight', 'encoder.block3.8.norm2.bias', 'encoder.block3.8.mlp.fc1.bias', 'encoder.block3.8.mlp.dwconv.dwconv.bias', 'encoder.block3.8.mlp.fc2.bias', 'encoder.block3.9.norm1.weight', 'encoder.block3.9.norm1.bias', 'encoder.block3.9.attn.q.bias', 'encoder.block3.9.attn.kv.bias', 'encoder.block3.9.attn.proj.bias', 'encoder.block3.9.attn.sr.bias', 'encoder.block3.9.attn.norm.weight', 'encoder.block3.9.attn.norm.bias', 'encoder.block3.9.norm2.weight', 'encoder.block3.9.norm2.bias', 'encoder.block3.9.mlp.fc1.bias', 'encoder.block3.9.mlp.dwconv.dwconv.bias', 'encoder.block3.9.mlp.fc2.bias', 'encoder.block3.10.norm1.weight', 'encoder.block3.10.norm1.bias', 'encoder.block3.10.attn.q.bias', 'encoder.block3.10.attn.kv.bias', 'encoder.block3.10.attn.proj.bias', 'encoder.block3.10.attn.sr.bias', 'encoder.block3.10.attn.norm.weight', 'encoder.block3.10.attn.norm.bias', 'encoder.block3.10.norm2.weight', 'encoder.block3.10.norm2.bias', 'encoder.block3.10.mlp.fc1.bias', 'encoder.block3.10.mlp.dwconv.dwconv.bias', 'encoder.block3.10.mlp.fc2.bias', 'encoder.block3.11.norm1.weight', 'encoder.block3.11.norm1.bias', 'encoder.block3.11.attn.q.bias', 'encoder.block3.11.attn.kv.bias', 'encoder.block3.11.attn.proj.bias', 'encoder.block3.11.attn.sr.bias', 'encoder.block3.11.attn.norm.weight', 'encoder.block3.11.attn.norm.bias', 'encoder.block3.11.norm2.weight', 'encoder.block3.11.norm2.bias', 'encoder.block3.11.mlp.fc1.bias', 'encoder.block3.11.mlp.dwconv.dwconv.bias', 'encoder.block3.11.mlp.fc2.bias', 'encoder.block3.12.norm1.weight', 'encoder.block3.12.norm1.bias', 'encoder.block3.12.attn.q.bias', 'encoder.block3.12.attn.kv.bias', 'encoder.block3.12.attn.proj.bias', 'encoder.block3.12.attn.sr.bias', 'encoder.block3.12.attn.norm.weight', 'encoder.block3.12.attn.norm.bias', 'encoder.block3.12.norm2.weight', 'encoder.block3.12.norm2.bias', 'encoder.block3.12.mlp.fc1.bias', 'encoder.block3.12.mlp.dwconv.dwconv.bias', 'encoder.block3.12.mlp.fc2.bias', 'encoder.block3.13.norm1.weight', 'encoder.block3.13.norm1.bias', 'encoder.block3.13.attn.q.bias', 'encoder.block3.13.attn.kv.bias', 'encoder.block3.13.attn.proj.bias', 'encoder.block3.13.attn.sr.bias', 'encoder.block3.13.attn.norm.weight', 'encoder.block3.13.attn.norm.bias', 'encoder.block3.13.norm2.weight', 'encoder.block3.13.norm2.bias', 'encoder.block3.13.mlp.fc1.bias', 'encoder.block3.13.mlp.dwconv.dwconv.bias', 'encoder.block3.13.mlp.fc2.bias', 'encoder.block3.14.norm1.weight', 'encoder.block3.14.norm1.bias', 'encoder.block3.14.attn.q.bias', 'encoder.block3.14.attn.kv.bias', 'encoder.block3.14.attn.proj.bias', 'encoder.block3.14.attn.sr.bias', 'encoder.block3.14.attn.norm.weight', 'encoder.block3.14.attn.norm.bias', 'encoder.block3.14.norm2.weight', 'encoder.block3.14.norm2.bias', 'encoder.block3.14.mlp.fc1.bias', 'encoder.block3.14.mlp.dwconv.dwconv.bias', 'encoder.block3.14.mlp.fc2.bias', 'encoder.block3.15.norm1.weight', 'encoder.block3.15.norm1.bias', 'encoder.block3.15.attn.q.bias', 'encoder.block3.15.attn.kv.bias', 'encoder.block3.15.attn.proj.bias', 'encoder.block3.15.attn.sr.bias', 'encoder.block3.15.attn.norm.weight', 'encoder.block3.15.attn.norm.bias', 'encoder.block3.15.norm2.weight', 'encoder.block3.15.norm2.bias', 'encoder.block3.15.mlp.fc1.bias', 'encoder.block3.15.mlp.dwconv.dwconv.bias', 'encoder.block3.15.mlp.fc2.bias', 'encoder.block3.16.norm1.weight', 'encoder.block3.16.norm1.bias', 'encoder.block3.16.attn.q.bias', 'encoder.block3.16.attn.kv.bias', 'encoder.block3.16.attn.proj.bias', 'encoder.block3.16.attn.sr.bias', 'encoder.block3.16.attn.norm.weight', 'encoder.block3.16.attn.norm.bias', 'encoder.block3.16.norm2.weight', 'encoder.block3.16.norm2.bias', 'encoder.block3.16.mlp.fc1.bias', 'encoder.block3.16.mlp.dwconv.dwconv.bias', 'encoder.block3.16.mlp.fc2.bias', 'encoder.block3.17.norm1.weight', 'encoder.block3.17.norm1.bias', 'encoder.block3.17.attn.q.bias', 'encoder.block3.17.attn.kv.bias', 'encoder.block3.17.attn.proj.bias', 'encoder.block3.17.attn.sr.bias', 'encoder.block3.17.attn.norm.weight', 'encoder.block3.17.attn.norm.bias', 'encoder.block3.17.norm2.weight', 'encoder.block3.17.norm2.bias', 'encoder.block3.17.mlp.fc1.bias', 'encoder.block3.17.mlp.dwconv.dwconv.bias', 'encoder.block3.17.mlp.fc2.bias', 'encoder.block3.18.norm1.weight', 'encoder.block3.18.norm1.bias', 'encoder.block3.18.attn.q.bias', 'encoder.block3.18.attn.kv.bias', 'encoder.block3.18.attn.proj.bias', 'encoder.block3.18.attn.sr.bias', 'encoder.block3.18.attn.norm.weight', 'encoder.block3.18.attn.norm.bias', 'encoder.block3.18.norm2.weight', 'encoder.block3.18.norm2.bias', 'encoder.block3.18.mlp.fc1.bias', 'encoder.block3.18.mlp.dwconv.dwconv.bias', 'encoder.block3.18.mlp.fc2.bias', 'encoder.block3.19.norm1.weight', 'encoder.block3.19.norm1.bias', 'encoder.block3.19.attn.q.bias', 'encoder.block3.19.attn.kv.bias', 'encoder.block3.19.attn.proj.bias', 'encoder.block3.19.attn.sr.bias', 'encoder.block3.19.attn.norm.weight', 'encoder.block3.19.attn.norm.bias', 'encoder.block3.19.norm2.weight', 'encoder.block3.19.norm2.bias', 'encoder.block3.19.mlp.fc1.bias', 'encoder.block3.19.mlp.dwconv.dwconv.bias', 'encoder.block3.19.mlp.fc2.bias', 'encoder.block3.20.norm1.weight', 'encoder.block3.20.norm1.bias', 'encoder.block3.20.attn.q.bias', 'encoder.block3.20.attn.kv.bias', 'encoder.block3.20.attn.proj.bias', 'encoder.block3.20.attn.sr.bias', 'encoder.block3.20.attn.norm.weight', 'encoder.block3.20.attn.norm.bias', 'encoder.block3.20.norm2.weight', 'encoder.block3.20.norm2.bias', 'encoder.block3.20.mlp.fc1.bias', 'encoder.block3.20.mlp.dwconv.dwconv.bias', 'encoder.block3.20.mlp.fc2.bias', 'encoder.block3.21.norm1.weight', 'encoder.block3.21.norm1.bias', 'encoder.block3.21.attn.q.bias', 'encoder.block3.21.attn.kv.bias', 'encoder.block3.21.attn.proj.bias', 'encoder.block3.21.attn.sr.bias', 'encoder.block3.21.attn.norm.weight', 'encoder.block3.21.attn.norm.bias', 'encoder.block3.21.norm2.weight', 'encoder.block3.21.norm2.bias', 'encoder.block3.21.mlp.fc1.bias', 'encoder.block3.21.mlp.dwconv.dwconv.bias', 'encoder.block3.21.mlp.fc2.bias', 'encoder.block3.22.norm1.weight', 'encoder.block3.22.norm1.bias', 'encoder.block3.22.attn.q.bias', 'encoder.block3.22.attn.kv.bias', 'encoder.block3.22.attn.proj.bias', 'encoder.block3.22.attn.sr.bias', 'encoder.block3.22.attn.norm.weight', 'encoder.block3.22.attn.norm.bias', 'encoder.block3.22.norm2.weight', 'encoder.block3.22.norm2.bias', 'encoder.block3.22.mlp.fc1.bias', 'encoder.block3.22.mlp.dwconv.dwconv.bias', 'encoder.block3.22.mlp.fc2.bias', 'encoder.block3.23.norm1.weight', 'encoder.block3.23.norm1.bias', 'encoder.block3.23.attn.q.bias', 'encoder.block3.23.attn.kv.bias', 'encoder.block3.23.attn.proj.bias', 'encoder.block3.23.attn.sr.bias', 'encoder.block3.23.attn.norm.weight', 'encoder.block3.23.attn.norm.bias', 'encoder.block3.23.norm2.weight', 'encoder.block3.23.norm2.bias', 'encoder.block3.23.mlp.fc1.bias', 'encoder.block3.23.mlp.dwconv.dwconv.bias', 'encoder.block3.23.mlp.fc2.bias', 'encoder.block3.24.norm1.weight', 'encoder.block3.24.norm1.bias', 'encoder.block3.24.attn.q.bias', 'encoder.block3.24.attn.kv.bias', 'encoder.block3.24.attn.proj.bias', 'encoder.block3.24.attn.sr.bias', 'encoder.block3.24.attn.norm.weight', 'encoder.block3.24.attn.norm.bias', 'encoder.block3.24.norm2.weight', 'encoder.block3.24.norm2.bias', 'encoder.block3.24.mlp.fc1.bias', 'encoder.block3.24.mlp.dwconv.dwconv.bias', 'encoder.block3.24.mlp.fc2.bias', 'encoder.block3.25.norm1.weight', 'encoder.block3.25.norm1.bias', 'encoder.block3.25.attn.q.bias', 'encoder.block3.25.attn.kv.bias', 'encoder.block3.25.attn.proj.bias', 'encoder.block3.25.attn.sr.bias', 'encoder.block3.25.attn.norm.weight', 'encoder.block3.25.attn.norm.bias', 'encoder.block3.25.norm2.weight', 'encoder.block3.25.norm2.bias', 'encoder.block3.25.mlp.fc1.bias', 'encoder.block3.25.mlp.dwconv.dwconv.bias', 'encoder.block3.25.mlp.fc2.bias', 'encoder.block3.26.norm1.weight', 'encoder.block3.26.norm1.bias', 'encoder.block3.26.attn.q.bias', 'encoder.block3.26.attn.kv.bias', 'encoder.block3.26.attn.proj.bias', 'encoder.block3.26.attn.sr.bias', 'encoder.block3.26.attn.norm.weight', 'encoder.block3.26.attn.norm.bias', 'encoder.block3.26.norm2.weight', 'encoder.block3.26.norm2.bias', 'encoder.block3.26.mlp.fc1.bias', 'encoder.block3.26.mlp.dwconv.dwconv.bias', 'encoder.block3.26.mlp.fc2.bias', 'encoder.block3.27.norm1.weight', 'encoder.block3.27.norm1.bias', 'encoder.block3.27.attn.q.bias', 'encoder.block3.27.attn.kv.bias', 'encoder.block3.27.attn.proj.bias', 'encoder.block3.27.attn.sr.bias', 'encoder.block3.27.attn.norm.weight', 'encoder.block3.27.attn.norm.bias', 'encoder.block3.27.norm2.weight', 'encoder.block3.27.norm2.bias', 'encoder.block3.27.mlp.fc1.bias', 'encoder.block3.27.mlp.dwconv.dwconv.bias', 'encoder.block3.27.mlp.fc2.bias', 'encoder.block3.28.norm1.weight', 'encoder.block3.28.norm1.bias', 'encoder.block3.28.attn.q.bias', 'encoder.block3.28.attn.kv.bias', 'encoder.block3.28.attn.proj.bias', 'encoder.block3.28.attn.sr.bias', 'encoder.block3.28.attn.norm.weight', 'encoder.block3.28.attn.norm.bias', 'encoder.block3.28.norm2.weight', 'encoder.block3.28.norm2.bias', 'encoder.block3.28.mlp.fc1.bias', 'encoder.block3.28.mlp.dwconv.dwconv.bias', 'encoder.block3.28.mlp.fc2.bias', 'encoder.block3.29.norm1.weight', 'encoder.block3.29.norm1.bias', 'encoder.block3.29.attn.q.bias', 'encoder.block3.29.attn.kv.bias', 'encoder.block3.29.attn.proj.bias', 'encoder.block3.29.attn.sr.bias', 'encoder.block3.29.attn.norm.weight', 'encoder.block3.29.attn.norm.bias', 'encoder.block3.29.norm2.weight', 'encoder.block3.29.norm2.bias', 'encoder.block3.29.mlp.fc1.bias', 'encoder.block3.29.mlp.dwconv.dwconv.bias', 'encoder.block3.29.mlp.fc2.bias', 'encoder.block3.30.norm1.weight', 'encoder.block3.30.norm1.bias', 'encoder.block3.30.attn.q.bias', 'encoder.block3.30.attn.kv.bias', 'encoder.block3.30.attn.proj.bias', 'encoder.block3.30.attn.sr.bias', 'encoder.block3.30.attn.norm.weight', 'encoder.block3.30.attn.norm.bias', 'encoder.block3.30.norm2.weight', 'encoder.block3.30.norm2.bias', 'encoder.block3.30.mlp.fc1.bias', 'encoder.block3.30.mlp.dwconv.dwconv.bias', 'encoder.block3.30.mlp.fc2.bias', 'encoder.block3.31.norm1.weight', 'encoder.block3.31.norm1.bias', 'encoder.block3.31.attn.q.bias', 'encoder.block3.31.attn.kv.bias', 'encoder.block3.31.attn.proj.bias', 'encoder.block3.31.attn.sr.bias', 'encoder.block3.31.attn.norm.weight', 'encoder.block3.31.attn.norm.bias', 'encoder.block3.31.norm2.weight', 'encoder.block3.31.norm2.bias', 'encoder.block3.31.mlp.fc1.bias', 'encoder.block3.31.mlp.dwconv.dwconv.bias', 'encoder.block3.31.mlp.fc2.bias', 'encoder.block3.32.norm1.weight', 'encoder.block3.32.norm1.bias', 'encoder.block3.32.attn.q.bias', 'encoder.block3.32.attn.kv.bias', 'encoder.block3.32.attn.proj.bias', 'encoder.block3.32.attn.sr.bias', 'encoder.block3.32.attn.norm.weight', 'encoder.block3.32.attn.norm.bias', 'encoder.block3.32.norm2.weight', 'encoder.block3.32.norm2.bias', 'encoder.block3.32.mlp.fc1.bias', 'encoder.block3.32.mlp.dwconv.dwconv.bias', 'encoder.block3.32.mlp.fc2.bias', 'encoder.block3.33.norm1.weight', 'encoder.block3.33.norm1.bias', 'encoder.block3.33.attn.q.bias', 'encoder.block3.33.attn.kv.bias', 'encoder.block3.33.attn.proj.bias', 'encoder.block3.33.attn.sr.bias', 'encoder.block3.33.attn.norm.weight', 'encoder.block3.33.attn.norm.bias', 'encoder.block3.33.norm2.weight', 'encoder.block3.33.norm2.bias', 'encoder.block3.33.mlp.fc1.bias', 'encoder.block3.33.mlp.dwconv.dwconv.bias', 'encoder.block3.33.mlp.fc2.bias', 'encoder.block3.34.norm1.weight', 'encoder.block3.34.norm1.bias', 'encoder.block3.34.attn.q.bias', 'encoder.block3.34.attn.kv.bias', 'encoder.block3.34.attn.proj.bias', 'encoder.block3.34.attn.sr.bias', 'encoder.block3.34.attn.norm.weight', 'encoder.block3.34.attn.norm.bias', 'encoder.block3.34.norm2.weight', 'encoder.block3.34.norm2.bias', 'encoder.block3.34.mlp.fc1.bias', 'encoder.block3.34.mlp.dwconv.dwconv.bias', 'encoder.block3.34.mlp.fc2.bias', 'encoder.block3.35.norm1.weight', 'encoder.block3.35.norm1.bias', 'encoder.block3.35.attn.q.bias', 'encoder.block3.35.attn.kv.bias', 'encoder.block3.35.attn.proj.bias', 'encoder.block3.35.attn.sr.bias', 'encoder.block3.35.attn.norm.weight', 'encoder.block3.35.attn.norm.bias', 'encoder.block3.35.norm2.weight', 'encoder.block3.35.norm2.bias', 'encoder.block3.35.mlp.fc1.bias', 'encoder.block3.35.mlp.dwconv.dwconv.bias', 'encoder.block3.35.mlp.fc2.bias', 'encoder.block3.36.norm1.weight', 'encoder.block3.36.norm1.bias', 'encoder.block3.36.attn.q.bias', 'encoder.block3.36.attn.kv.bias', 'encoder.block3.36.attn.proj.bias', 'encoder.block3.36.attn.sr.bias', 'encoder.block3.36.attn.norm.weight', 'encoder.block3.36.attn.norm.bias', 'encoder.block3.36.norm2.weight', 'encoder.block3.36.norm2.bias', 'encoder.block3.36.mlp.fc1.bias', 'encoder.block3.36.mlp.dwconv.dwconv.bias', 'encoder.block3.36.mlp.fc2.bias', 'encoder.block3.37.norm1.weight', 'encoder.block3.37.norm1.bias', 'encoder.block3.37.attn.q.bias', 'encoder.block3.37.attn.kv.bias', 'encoder.block3.37.attn.proj.bias', 'encoder.block3.37.attn.sr.bias', 'encoder.block3.37.attn.norm.weight', 'encoder.block3.37.attn.norm.bias', 'encoder.block3.37.norm2.weight', 'encoder.block3.37.norm2.bias', 'encoder.block3.37.mlp.fc1.bias', 'encoder.block3.37.mlp.dwconv.dwconv.bias', 'encoder.block3.37.mlp.fc2.bias', 'encoder.block3.38.norm1.weight', 'encoder.block3.38.norm1.bias', 'encoder.block3.38.attn.q.bias', 'encoder.block3.38.attn.kv.bias', 'encoder.block3.38.attn.proj.bias', 'encoder.block3.38.attn.sr.bias', 'encoder.block3.38.attn.norm.weight', 'encoder.block3.38.attn.norm.bias', 'encoder.block3.38.norm2.weight', 'encoder.block3.38.norm2.bias', 'encoder.block3.38.mlp.fc1.bias', 'encoder.block3.38.mlp.dwconv.dwconv.bias', 'encoder.block3.38.mlp.fc2.bias', 'encoder.block3.39.norm1.weight', 'encoder.block3.39.norm1.bias', 'encoder.block3.39.attn.q.bias', 'encoder.block3.39.attn.kv.bias', 'encoder.block3.39.attn.proj.bias', 'encoder.block3.39.attn.sr.bias', 'encoder.block3.39.attn.norm.weight', 'encoder.block3.39.attn.norm.bias', 'encoder.block3.39.norm2.weight', 'encoder.block3.39.norm2.bias', 'encoder.block3.39.mlp.fc1.bias', 'encoder.block3.39.mlp.dwconv.dwconv.bias', 'encoder.block3.39.mlp.fc2.bias', 'encoder.norm3.weight', 'encoder.norm3.bias', 'encoder.block4.0.norm1.weight', 'encoder.block4.0.norm1.bias', 'encoder.block4.0.attn.q.bias', 'encoder.block4.0.attn.kv.bias', 'encoder.block4.0.attn.proj.bias', 'encoder.block4.0.norm2.weight', 'encoder.block4.0.norm2.bias', 'encoder.block4.0.mlp.fc1.bias', 'encoder.block4.0.mlp.dwconv.dwconv.bias', 'encoder.block4.0.mlp.fc2.bias', 'encoder.block4.1.norm1.weight', 'encoder.block4.1.norm1.bias', 'encoder.block4.1.attn.q.bias', 'encoder.block4.1.attn.kv.bias', 'encoder.block4.1.attn.proj.bias', 'encoder.block4.1.norm2.weight', 'encoder.block4.1.norm2.bias', 'encoder.block4.1.mlp.fc1.bias', 'encoder.block4.1.mlp.dwconv.dwconv.bias', 'encoder.block4.1.mlp.fc2.bias', 'encoder.block4.2.norm1.weight', 'encoder.block4.2.norm1.bias', 'encoder.block4.2.attn.q.bias', 'encoder.block4.2.attn.kv.bias', 'encoder.block4.2.attn.proj.bias', 'encoder.block4.2.norm2.weight', 'encoder.block4.2.norm2.bias', 'encoder.block4.2.mlp.fc1.bias', 'encoder.block4.2.mlp.dwconv.dwconv.bias', 'encoder.block4.2.mlp.fc2.bias', 'encoder.norm4.weight', 'encoder.norm4.bias', 'decoder.0.bias']
[2022-11-11 17:50:32 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.mask_token', 'encoder.patch_embed1.proj.weight', 'encoder.patch_embed2.proj.weight', 'encoder.patch_embed3.proj.weight', 'encoder.patch_embed4.proj.weight', 'encoder.block1.0.attn.q.weight', 'encoder.block1.0.attn.kv.weight', 'encoder.block1.0.attn.proj.weight', 'encoder.block1.0.attn.sr.weight', 'encoder.block1.0.mlp.fc1.weight', 'encoder.block1.0.mlp.dwconv.dwconv.weight', 'encoder.block1.0.mlp.fc2.weight', 'encoder.block1.1.attn.q.weight', 'encoder.block1.1.attn.kv.weight', 'encoder.block1.1.attn.proj.weight', 'encoder.block1.1.attn.sr.weight', 'encoder.block1.1.mlp.fc1.weight', 'encoder.block1.1.mlp.dwconv.dwconv.weight', 'encoder.block1.1.mlp.fc2.weight', 'encoder.block1.2.attn.q.weight', 'encoder.block1.2.attn.kv.weight', 'encoder.block1.2.attn.proj.weight', 'encoder.block1.2.attn.sr.weight', 'encoder.block1.2.mlp.fc1.weight', 'encoder.block1.2.mlp.dwconv.dwconv.weight', 'encoder.block1.2.mlp.fc2.weight', 'encoder.block2.0.attn.q.weight', 'encoder.block2.0.attn.kv.weight', 'encoder.block2.0.attn.proj.weight', 'encoder.block2.0.attn.sr.weight', 'encoder.block2.0.mlp.fc1.weight', 'encoder.block2.0.mlp.dwconv.dwconv.weight', 'encoder.block2.0.mlp.fc2.weight', 'encoder.block2.1.attn.q.weight', 'encoder.block2.1.attn.kv.weight', 'encoder.block2.1.attn.proj.weight', 'encoder.block2.1.attn.sr.weight', 'encoder.block2.1.mlp.fc1.weight', 'encoder.block2.1.mlp.dwconv.dwconv.weight', 'encoder.block2.1.mlp.fc2.weight', 'encoder.block2.2.attn.q.weight', 'encoder.block2.2.attn.kv.weight', 'encoder.block2.2.attn.proj.weight', 'encoder.block2.2.attn.sr.weight', 'encoder.block2.2.mlp.fc1.weight', 'encoder.block2.2.mlp.dwconv.dwconv.weight', 'encoder.block2.2.mlp.fc2.weight', 'encoder.block2.3.attn.q.weight', 'encoder.block2.3.attn.kv.weight', 'encoder.block2.3.attn.proj.weight', 'encoder.block2.3.attn.sr.weight', 'encoder.block2.3.mlp.fc1.weight', 'encoder.block2.3.mlp.dwconv.dwconv.weight', 'encoder.block2.3.mlp.fc2.weight', 'encoder.block2.4.attn.q.weight', 'encoder.block2.4.attn.kv.weight', 'encoder.block2.4.attn.proj.weight', 'encoder.block2.4.attn.sr.weight', 'encoder.block2.4.mlp.fc1.weight', 'encoder.block2.4.mlp.dwconv.dwconv.weight', 'encoder.block2.4.mlp.fc2.weight', 'encoder.block2.5.attn.q.weight', 'encoder.block2.5.attn.kv.weight', 'encoder.block2.5.attn.proj.weight', 'encoder.block2.5.attn.sr.weight', 'encoder.block2.5.mlp.fc1.weight', 'encoder.block2.5.mlp.dwconv.dwconv.weight', 'encoder.block2.5.mlp.fc2.weight', 'encoder.block3.0.attn.q.weight', 'encoder.block3.0.attn.kv.weight', 'encoder.block3.0.attn.proj.weight', 'encoder.block3.0.attn.sr.weight', 'encoder.block3.0.mlp.fc1.weight', 'encoder.block3.0.mlp.dwconv.dwconv.weight', 'encoder.block3.0.mlp.fc2.weight', 'encoder.block3.1.attn.q.weight', 'encoder.block3.1.attn.kv.weight', 'encoder.block3.1.attn.proj.weight', 'encoder.block3.1.attn.sr.weight', 'encoder.block3.1.mlp.fc1.weight', 'encoder.block3.1.mlp.dwconv.dwconv.weight', 'encoder.block3.1.mlp.fc2.weight', 'encoder.block3.2.attn.q.weight', 'encoder.block3.2.attn.kv.weight', 'encoder.block3.2.attn.proj.weight', 'encoder.block3.2.attn.sr.weight', 'encoder.block3.2.mlp.fc1.weight', 'encoder.block3.2.mlp.dwconv.dwconv.weight', 'encoder.block3.2.mlp.fc2.weight', 'encoder.block3.3.attn.q.weight', 'encoder.block3.3.attn.kv.weight', 'encoder.block3.3.attn.proj.weight', 'encoder.block3.3.attn.sr.weight', 'encoder.block3.3.mlp.fc1.weight', 'encoder.block3.3.mlp.dwconv.dwconv.weight', 'encoder.block3.3.mlp.fc2.weight', 'encoder.block3.4.attn.q.weight', 'encoder.block3.4.attn.kv.weight', 'encoder.block3.4.attn.proj.weight', 'encoder.block3.4.attn.sr.weight', 'encoder.block3.4.mlp.fc1.weight', 'encoder.block3.4.mlp.dwconv.dwconv.weight', 'encoder.block3.4.mlp.fc2.weight', 'encoder.block3.5.attn.q.weight', 'encoder.block3.5.attn.kv.weight', 'encoder.block3.5.attn.proj.weight', 'encoder.block3.5.attn.sr.weight', 'encoder.block3.5.mlp.fc1.weight', 'encoder.block3.5.mlp.dwconv.dwconv.weight', 'encoder.block3.5.mlp.fc2.weight', 'encoder.block3.6.attn.q.weight', 'encoder.block3.6.attn.kv.weight', 'encoder.block3.6.attn.proj.weight', 'encoder.block3.6.attn.sr.weight', 'encoder.block3.6.mlp.fc1.weight', 'encoder.block3.6.mlp.dwconv.dwconv.weight', 'encoder.block3.6.mlp.fc2.weight', 'encoder.block3.7.attn.q.weight', 'encoder.block3.7.attn.kv.weight', 'encoder.block3.7.attn.proj.weight', 'encoder.block3.7.attn.sr.weight', 'encoder.block3.7.mlp.fc1.weight', 'encoder.block3.7.mlp.dwconv.dwconv.weight', 'encoder.block3.7.mlp.fc2.weight', 'encoder.block3.8.attn.q.weight', 'encoder.block3.8.attn.kv.weight', 'encoder.block3.8.attn.proj.weight', 'encoder.block3.8.attn.sr.weight', 'encoder.block3.8.mlp.fc1.weight', 'encoder.block3.8.mlp.dwconv.dwconv.weight', 'encoder.block3.8.mlp.fc2.weight', 'encoder.block3.9.attn.q.weight', 'encoder.block3.9.attn.kv.weight', 'encoder.block3.9.attn.proj.weight', 'encoder.block3.9.attn.sr.weight', 'encoder.block3.9.mlp.fc1.weight', 'encoder.block3.9.mlp.dwconv.dwconv.weight', 'encoder.block3.9.mlp.fc2.weight', 'encoder.block3.10.attn.q.weight', 'encoder.block3.10.attn.kv.weight', 'encoder.block3.10.attn.proj.weight', 'encoder.block3.10.attn.sr.weight', 'encoder.block3.10.mlp.fc1.weight', 'encoder.block3.10.mlp.dwconv.dwconv.weight', 'encoder.block3.10.mlp.fc2.weight', 'encoder.block3.11.attn.q.weight', 'encoder.block3.11.attn.kv.weight', 'encoder.block3.11.attn.proj.weight', 'encoder.block3.11.attn.sr.weight', 'encoder.block3.11.mlp.fc1.weight', 'encoder.block3.11.mlp.dwconv.dwconv.weight', 'encoder.block3.11.mlp.fc2.weight', 'encoder.block3.12.attn.q.weight', 'encoder.block3.12.attn.kv.weight', 'encoder.block3.12.attn.proj.weight', 'encoder.block3.12.attn.sr.weight', 'encoder.block3.12.mlp.fc1.weight', 'encoder.block3.12.mlp.dwconv.dwconv.weight', 'encoder.block3.12.mlp.fc2.weight', 'encoder.block3.13.attn.q.weight', 'encoder.block3.13.attn.kv.weight', 'encoder.block3.13.attn.proj.weight', 'encoder.block3.13.attn.sr.weight', 'encoder.block3.13.mlp.fc1.weight', 'encoder.block3.13.mlp.dwconv.dwconv.weight', 'encoder.block3.13.mlp.fc2.weight', 'encoder.block3.14.attn.q.weight', 'encoder.block3.14.attn.kv.weight', 'encoder.block3.14.attn.proj.weight', 'encoder.block3.14.attn.sr.weight', 'encoder.block3.14.mlp.fc1.weight', 'encoder.block3.14.mlp.dwconv.dwconv.weight', 'encoder.block3.14.mlp.fc2.weight', 'encoder.block3.15.attn.q.weight', 'encoder.block3.15.attn.kv.weight', 'encoder.block3.15.attn.proj.weight', 'encoder.block3.15.attn.sr.weight', 'encoder.block3.15.mlp.fc1.weight', 'encoder.block3.15.mlp.dwconv.dwconv.weight', 'encoder.block3.15.mlp.fc2.weight', 'encoder.block3.16.attn.q.weight', 'encoder.block3.16.attn.kv.weight', 'encoder.block3.16.attn.proj.weight', 'encoder.block3.16.attn.sr.weight', 'encoder.block3.16.mlp.fc1.weight', 'encoder.block3.16.mlp.dwconv.dwconv.weight', 'encoder.block3.16.mlp.fc2.weight', 'encoder.block3.17.attn.q.weight', 'encoder.block3.17.attn.kv.weight', 'encoder.block3.17.attn.proj.weight', 'encoder.block3.17.attn.sr.weight', 'encoder.block3.17.mlp.fc1.weight', 'encoder.block3.17.mlp.dwconv.dwconv.weight', 'encoder.block3.17.mlp.fc2.weight', 'encoder.block3.18.attn.q.weight', 'encoder.block3.18.attn.kv.weight', 'encoder.block3.18.attn.proj.weight', 'encoder.block3.18.attn.sr.weight', 'encoder.block3.18.mlp.fc1.weight', 'encoder.block3.18.mlp.dwconv.dwconv.weight', 'encoder.block3.18.mlp.fc2.weight', 'encoder.block3.19.attn.q.weight', 'encoder.block3.19.attn.kv.weight', 'encoder.block3.19.attn.proj.weight', 'encoder.block3.19.attn.sr.weight', 'encoder.block3.19.mlp.fc1.weight', 'encoder.block3.19.mlp.dwconv.dwconv.weight', 'encoder.block3.19.mlp.fc2.weight', 'encoder.block3.20.attn.q.weight', 'encoder.block3.20.attn.kv.weight', 'encoder.block3.20.attn.proj.weight', 'encoder.block3.20.attn.sr.weight', 'encoder.block3.20.mlp.fc1.weight', 'encoder.block3.20.mlp.dwconv.dwconv.weight', 'encoder.block3.20.mlp.fc2.weight', 'encoder.block3.21.attn.q.weight', 'encoder.block3.21.attn.kv.weight', 'encoder.block3.21.attn.proj.weight', 'encoder.block3.21.attn.sr.weight', 'encoder.block3.21.mlp.fc1.weight', 'encoder.block3.21.mlp.dwconv.dwconv.weight', 'encoder.block3.21.mlp.fc2.weight', 'encoder.block3.22.attn.q.weight', 'encoder.block3.22.attn.kv.weight', 'encoder.block3.22.attn.proj.weight', 'encoder.block3.22.attn.sr.weight', 'encoder.block3.22.mlp.fc1.weight', 'encoder.block3.22.mlp.dwconv.dwconv.weight', 'encoder.block3.22.mlp.fc2.weight', 'encoder.block3.23.attn.q.weight', 'encoder.block3.23.attn.kv.weight', 'encoder.block3.23.attn.proj.weight', 'encoder.block3.23.attn.sr.weight', 'encoder.block3.23.mlp.fc1.weight', 'encoder.block3.23.mlp.dwconv.dwconv.weight', 'encoder.block3.23.mlp.fc2.weight', 'encoder.block3.24.attn.q.weight', 'encoder.block3.24.attn.kv.weight', 'encoder.block3.24.attn.proj.weight', 'encoder.block3.24.attn.sr.weight', 'encoder.block3.24.mlp.fc1.weight', 'encoder.block3.24.mlp.dwconv.dwconv.weight', 'encoder.block3.24.mlp.fc2.weight', 'encoder.block3.25.attn.q.weight', 'encoder.block3.25.attn.kv.weight', 'encoder.block3.25.attn.proj.weight', 'encoder.block3.25.attn.sr.weight', 'encoder.block3.25.mlp.fc1.weight', 'encoder.block3.25.mlp.dwconv.dwconv.weight', 'encoder.block3.25.mlp.fc2.weight', 'encoder.block3.26.attn.q.weight', 'encoder.block3.26.attn.kv.weight', 'encoder.block3.26.attn.proj.weight', 'encoder.block3.26.attn.sr.weight', 'encoder.block3.26.mlp.fc1.weight', 'encoder.block3.26.mlp.dwconv.dwconv.weight', 'encoder.block3.26.mlp.fc2.weight', 'encoder.block3.27.attn.q.weight', 'encoder.block3.27.attn.kv.weight', 'encoder.block3.27.attn.proj.weight', 'encoder.block3.27.attn.sr.weight', 'encoder.block3.27.mlp.fc1.weight', 'encoder.block3.27.mlp.dwconv.dwconv.weight', 'encoder.block3.27.mlp.fc2.weight', 'encoder.block3.28.attn.q.weight', 'encoder.block3.28.attn.kv.weight', 'encoder.block3.28.attn.proj.weight', 'encoder.block3.28.attn.sr.weight', 'encoder.block3.28.mlp.fc1.weight', 'encoder.block3.28.mlp.dwconv.dwconv.weight', 'encoder.block3.28.mlp.fc2.weight', 'encoder.block3.29.attn.q.weight', 'encoder.block3.29.attn.kv.weight', 'encoder.block3.29.attn.proj.weight', 'encoder.block3.29.attn.sr.weight', 'encoder.block3.29.mlp.fc1.weight', 'encoder.block3.29.mlp.dwconv.dwconv.weight', 'encoder.block3.29.mlp.fc2.weight', 'encoder.block3.30.attn.q.weight', 'encoder.block3.30.attn.kv.weight', 'encoder.block3.30.attn.proj.weight', 'encoder.block3.30.attn.sr.weight', 'encoder.block3.30.mlp.fc1.weight', 'encoder.block3.30.mlp.dwconv.dwconv.weight', 'encoder.block3.30.mlp.fc2.weight', 'encoder.block3.31.attn.q.weight', 'encoder.block3.31.attn.kv.weight', 'encoder.block3.31.attn.proj.weight', 'encoder.block3.31.attn.sr.weight', 'encoder.block3.31.mlp.fc1.weight', 'encoder.block3.31.mlp.dwconv.dwconv.weight', 'encoder.block3.31.mlp.fc2.weight', 'encoder.block3.32.attn.q.weight', 'encoder.block3.32.attn.kv.weight', 'encoder.block3.32.attn.proj.weight', 'encoder.block3.32.attn.sr.weight', 'encoder.block3.32.mlp.fc1.weight', 'encoder.block3.32.mlp.dwconv.dwconv.weight', 'encoder.block3.32.mlp.fc2.weight', 'encoder.block3.33.attn.q.weight', 'encoder.block3.33.attn.kv.weight', 'encoder.block3.33.attn.proj.weight', 'encoder.block3.33.attn.sr.weight', 'encoder.block3.33.mlp.fc1.weight', 'encoder.block3.33.mlp.dwconv.dwconv.weight', 'encoder.block3.33.mlp.fc2.weight', 'encoder.block3.34.attn.q.weight', 'encoder.block3.34.attn.kv.weight', 'encoder.block3.34.attn.proj.weight', 'encoder.block3.34.attn.sr.weight', 'encoder.block3.34.mlp.fc1.weight', 'encoder.block3.34.mlp.dwconv.dwconv.weight', 'encoder.block3.34.mlp.fc2.weight', 'encoder.block3.35.attn.q.weight', 'encoder.block3.35.attn.kv.weight', 'encoder.block3.35.attn.proj.weight', 'encoder.block3.35.attn.sr.weight', 'encoder.block3.35.mlp.fc1.weight', 'encoder.block3.35.mlp.dwconv.dwconv.weight', 'encoder.block3.35.mlp.fc2.weight', 'encoder.block3.36.attn.q.weight', 'encoder.block3.36.attn.kv.weight', 'encoder.block3.36.attn.proj.weight', 'encoder.block3.36.attn.sr.weight', 'encoder.block3.36.mlp.fc1.weight', 'encoder.block3.36.mlp.dwconv.dwconv.weight', 'encoder.block3.36.mlp.fc2.weight', 'encoder.block3.37.attn.q.weight', 'encoder.block3.37.attn.kv.weight', 'encoder.block3.37.attn.proj.weight', 'encoder.block3.37.attn.sr.weight', 'encoder.block3.37.mlp.fc1.weight', 'encoder.block3.37.mlp.dwconv.dwconv.weight', 'encoder.block3.37.mlp.fc2.weight', 'encoder.block3.38.attn.q.weight', 'encoder.block3.38.attn.kv.weight', 'encoder.block3.38.attn.proj.weight', 'encoder.block3.38.attn.sr.weight', 'encoder.block3.38.mlp.fc1.weight', 'encoder.block3.38.mlp.dwconv.dwconv.weight', 'encoder.block3.38.mlp.fc2.weight', 'encoder.block3.39.attn.q.weight', 'encoder.block3.39.attn.kv.weight', 'encoder.block3.39.attn.proj.weight', 'encoder.block3.39.attn.sr.weight', 'encoder.block3.39.mlp.fc1.weight', 'encoder.block3.39.mlp.dwconv.dwconv.weight', 'encoder.block3.39.mlp.fc2.weight', 'encoder.block4.0.attn.q.weight', 'encoder.block4.0.attn.kv.weight', 'encoder.block4.0.attn.proj.weight', 'encoder.block4.0.mlp.fc1.weight', 'encoder.block4.0.mlp.dwconv.dwconv.weight', 'encoder.block4.0.mlp.fc2.weight', 'encoder.block4.1.attn.q.weight', 'encoder.block4.1.attn.kv.weight', 'encoder.block4.1.attn.proj.weight', 'encoder.block4.1.mlp.fc1.weight', 'encoder.block4.1.mlp.dwconv.dwconv.weight', 'encoder.block4.1.mlp.fc2.weight', 'encoder.block4.2.attn.q.weight', 'encoder.block4.2.attn.kv.weight', 'encoder.block4.2.attn.proj.weight', 'encoder.block4.2.mlp.fc1.weight', 'encoder.block4.2.mlp.dwconv.dwconv.weight', 'encoder.block4.2.mlp.fc2.weight', 'decoder.0.weight']
[2022-11-11 17:50:32 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 1.953125e-07
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 1.953125e-07
    maximize: False
    weight_decay: 0.0
)
[2022-11-11 17:50:32 simmim_pretrain] (main_simmim.py 83): INFO number of params: 83022016
[2022-11-11 17:50:32 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep: []
[2022-11-11 17:50:32 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in /home/r/SimMIM-main/output/simmim_pretrain/simmim_pretrain__mit_b5__img512__800ep, ignoring auto resume
[2022-11-11 17:50:32 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-11-11 17:50:33 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][0/22210]	eta 10:04:04 lr 0.000000	time 1.6319 (1.6319)	loss 0.3499 (0.3499)	grad_norm 0.2383 (0.2383)	mem 3086MB
[2022-11-11 17:50:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][10/22210]	eta 2:32:19 lr 0.000000	time 0.2875 (0.4117)	loss 0.4709 (0.3700)	grad_norm 0.3715 (0.2598)	mem 3387MB
[2022-11-11 17:50:39 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][20/22210]	eta 2:11:11 lr 0.000000	time 0.2890 (0.3547)	loss 0.3752 (0.3693)	grad_norm 0.3078 (0.2734)	mem 3387MB
[2022-11-11 17:50:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][30/22210]	eta 2:03:13 lr 0.000000	time 0.2862 (0.3333)	loss 0.3207 (0.3674)	grad_norm 0.2787 (0.2816)	mem 3387MB
[2022-11-11 17:50:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][40/22210]	eta 1:59:16 lr 0.000000	time 0.2876 (0.3228)	loss 0.4349 (0.3583)	grad_norm 0.4086 (0.2871)	mem 3387MB
[2022-11-11 17:50:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/800][50/22210]	eta 1:57:03 lr 0.000000	time 0.2983 (0.3169)	loss 0.2654 (0.3545)	grad_norm 0.2902 (0.2840)	mem 3387MB
